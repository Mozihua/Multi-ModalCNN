{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 176, 208, 1)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 176, 208, 64)      640       \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 176, 208, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 88, 104, 64)       0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 88, 104, 128)      73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 88, 104, 128)      147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 44, 52, 128)       0         \n",
      "=================================================================\n",
      "Total params: 259,008\n",
      "Trainable params: 259,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/avell/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:36: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"bl...)`\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "from keras.models import Sequential, Model \n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D\n",
    "from keras import backend as k \n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
    "from keras.applications.vgg19 import VGG19\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Flatten, Dense\n",
    "from keras.models import Model\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.utils import to_categorical\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "img_width, img_height =  176, 208\n",
    "\n",
    "### Build the network \n",
    "img_input = Input(shape=(img_width, img_height, 1))\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(img_input)\n",
    "x = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n",
    "\n",
    "# Block 2\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\n",
    "x = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\n",
    "x = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n",
    "\n",
    "model = Model(input = img_input, output = x)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['input_1',\n",
       " 'block1_conv1',\n",
       " 'block1_conv2',\n",
       " 'block1_pool',\n",
       " 'block2_conv1',\n",
       " 'block2_conv2',\n",
       " 'block2_pool']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_dict = dict([(layer.name, layer) for layer in model.layers])\n",
    "[layer.name for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['block1_conv1',\n",
       " 'block1_conv2',\n",
       " 'block1_pool',\n",
       " 'block2_conv1',\n",
       " 'block2_conv2',\n",
       " 'block2_pool',\n",
       " 'block3_conv1',\n",
       " 'block3_conv2',\n",
       " 'block3_conv3',\n",
       " 'block3_conv4',\n",
       " 'block3_pool',\n",
       " 'block4_conv1',\n",
       " 'block4_conv2',\n",
       " 'block4_conv3',\n",
       " 'block4_conv4',\n",
       " 'block4_pool',\n",
       " 'block5_conv1',\n",
       " 'block5_conv2',\n",
       " 'block5_conv3',\n",
       " 'block5_conv4',\n",
       " 'block5_pool',\n",
       " 'fc1',\n",
       " 'fc2',\n",
       " 'flatten',\n",
       " 'predictions']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h5py\n",
    "weights_path = '/home/avell/Downloads/vgg19_weights.h5' # ('https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels.h5)\n",
    "f = h5py.File(weights_path)\n",
    "list(f.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'block1_conv1_W_1:0', b'block1_conv1_b_1:0'], \n",
       "      dtype='|S18')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all the layer names which are in the model.\n",
    "layer_names = [layer.name for layer in model.layers]\n",
    "\n",
    "f[\"block1_conv1\"].attrs[\"weight_names\"]\n",
    "# \"\"\"\n",
    "# # Here we are extracting model_weights for each and every layer from the .h5 file\n",
    "\n",
    "# >>> f[\"model_weights\"][\"block1_conv1\"].attrs[\"weight_names\"]\n",
    "# array([b'block1_conv1/kernel:0', b'block1_conv1/bias:0'], \n",
    "#       dtype='|S21')\n",
    "# # we are assiging this array to weight_names below \n",
    "\n",
    "# >>> f[\"model_weights\"][\"block1_conv1\"][\"block1_conv1/kernel:0]\n",
    "# <HDF5 dataset \"kernel:0\": shape (3, 3, 3, 64), type \"<f4\">\n",
    "# # The list comprehension (weights) stores these two weights and bias of both the layers \n",
    "\n",
    "# >>>layer_names.index(\"block1_conv1\")\n",
    "# 1\n",
    "\n",
    "# >>> model.layers[1].set_weights(weights)\n",
    "# # This will set the weights for that particular layer.\n",
    "# With a for loop we can set_weights for the entire network.\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fcacaf77dd8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fcaccc632b0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fcabcf61828>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fcabcf618d0>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fcabcf70208>,\n",
       " <keras.layers.convolutional.Conv2D at 0x7fcabcf04390>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x7fcabcf04860>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in layer_dict.keys():\n",
    "    if i == 'input_1':\n",
    "        break\n",
    "    weight_names = f[i].attrs[\"weight_names\"]\n",
    "    weights = [f[i][j] for j in weight_names]\n",
    "    index = layer_names.index(i)\n",
    "    model.layers[index].set_weights(weights)\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import glob\n",
    "import subprocess\n",
    "from skimage.io import imread\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "def call_shell(cmd, *argv):\n",
    "    list_ = [cmd];    [list_.append(arg) for arg in argv]\n",
    "    out = subprocess.run(list_, stdout=subprocess.PIPE)\n",
    "    return out.stdout.decode('utf-8')\n",
    "\n",
    "DEST = '/media/avell/Novo volume/CV_SARGITAL/'\n",
    "\n",
    "def features_extraction(fold):\n",
    "    reshape_im = lambda im: np.expand_dims(np.reshape(im,(im.shape[0],im.shape[1],1)),axis=0)\n",
    "    reshape = lambda array: [np.array([i.ravel(),]) for i in array ]\n",
    "\n",
    "    location_0 = fold+'0.0/'\n",
    "    location_1 = fold+'1.0/'\n",
    "    \n",
    "    files_0 = call_shell('ls',location_0).split('\\n')[:-1]\n",
    "    files_1 = call_shell('ls',location_1).split('\\n')[:-1]\n",
    "\n",
    "    features_1 = []\n",
    "    features_0 = []\n",
    "    \n",
    "    for i in files_0:\n",
    "        im_0 = imread(location_0+i)\n",
    "        im_0 = reshape_im(im_0)\n",
    "        features_0.append(model.predict(im_0))    \n",
    "    for i in files_1:\n",
    "        im_1 = imread(location_1+i)\n",
    "        im_1 = reshape_im(im_1)\n",
    "        features_1.append(model.predict(im_1))\n",
    "        \n",
    "    mt_f0 = reshape(features_0)\n",
    "    mt_f1 = reshape(features_1)\n",
    "\n",
    "    mt_f0 = np.array(mt_f0).reshape(len(mt_f0),mt_f0[0].shape[1])\n",
    "    mt_f1 = np.array(mt_f1).reshape(len(mt_f1),mt_f1[0].shape[1])\n",
    "\n",
    "    mt_f0 = np.hstack([mt_f0 ,np.zeros([mt_f0.shape[0],1])])\n",
    "    mt_f1 = np.hstack([mt_f1 ,np.ones([mt_f1.shape[0],1])])\n",
    "\n",
    "    df = pd.DataFrame(np.vstack([mt_f0,mt_f1]))\n",
    "    \n",
    "    return df\n",
    "        \n",
    "## collect these features and create a dataframe and train a classfier on top of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_features_train = []\n",
    "k_features_test = []\n",
    "\n",
    "for i in range(10):\n",
    "    fold_tr = DEST+'fold_{}/train/'.format(i)\n",
    "    fold_te = DEST+'fold_{}/test/'.format(i)\n",
    "    k_features_train.append(features_extraction(fold_tr))\n",
    "    k_features_test.append(features_extraction(fold_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean\t| Precision |\t| Recall |\t| F-score |\t| Accuracy |\t| Specificity |\n",
      "| 0.6899658674658674 || 0.6839664502164502 || 0.6721114588960193 || 0.6789473684210525 || 0.6231818181818183 |\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score,confusion_matrix, classification_report\n",
    "import csv\n",
    "\n",
    "def meanMetrics(predList, yList):\n",
    "    PR, RC, F1, ACC, SP = [],[],[],[],[]\n",
    "    for p, y in zip(predList, yList):\n",
    "        pr,rl,f1,sp = precision_recall_fscore_support(p, y)\n",
    "        acc = accuracy_score(p, y)\n",
    "        tn, fp, fn, tp = confusion_matrix(p, y).ravel()\n",
    "        specificity = 1.0*tn / (tn+fp)\n",
    "        PR.append(pr); RC.append(rl); F1.append(f1);\n",
    "        ACC.append(acc); SP.append(specificity)\n",
    "    PR, RC, F1, ACC, SP = np.array(PR),np.array(RC),np.array(F1),np.array(ACC),np.array(SP)\n",
    "    print('Mean\\t| Precision |\\t| Recall |\\t| F-score |\\t| Accuracy |\\t| Specificity |')\n",
    "    exit = '| {} || {} || {} || {} || {} |'\n",
    "    print(exit.format(np.mean(PR),np.mean(RC),np.mean(F1),np.mean(ACC),np.mean(SP)))\n",
    "\n",
    "\n",
    "axialPred, y_labels = [],[]\n",
    "\n",
    "for i in range(10):\n",
    "    c_y = k_features_test[0].shape[1]\n",
    "    x_test = k_features_test[i].drop(c_y-1,axis=1)\n",
    "    x_train = k_features_train[i].drop(c_y-1,axis=1)\n",
    "    y_test = k_features_test[i][c_y-1]\n",
    "    y_train = k_features_train[i][c_y-1]\n",
    "    \n",
    "    svm = SVC(C=100,gamma=2e-6)\n",
    "    svm.fit(x_train,y_train)\n",
    "    pred = svm.predict(x_test)\n",
    "    \n",
    "    fold = DEST+'fold_{}'.format(i)\n",
    "    \n",
    "    pr,rl,f1,sp = precision_recall_fscore_support(pred, y_test)\n",
    "    acc = accuracy_score(pred, y_test)\n",
    "    tn, fp, fn, tp = confusion_matrix(pred, y_test).ravel()\n",
    "    specificity = 1.0*tn / (tn+fp+1e-16)\n",
    "    \n",
    "    with open(fold+'/fold_{}_metrics.csv'.format(i),'w') as csvFile:\n",
    "        filewriter = csv.writer(csvFile)\n",
    "        filewriter.writerow(['class','precision','recall','fscore','support','accuracy','specificity'])\n",
    "        filewriter.writerow([0,pr[0],rl[0],f1[0],sp[0],'-','-'])\n",
    "        filewriter.writerow([1,pr[1],rl[1],f1[1],sp[1],'-','-'])\n",
    "        filewriter.writerow(['-','-','-','-','-',acc,specificity])\n",
    "    csvFile.close()\n",
    "    \n",
    "    np.save(fold+'/fold_{}_pred_labels.npy'.format(i),pred)\n",
    "    \n",
    "    axialPred.append(pred)\n",
    "    y_labels.append(y_test)\n",
    "\n",
    "meanMetrics(axialPred,y_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEST = '/media/avell/Novo volume/CV_AXIAL/'\n",
    "EXT = '_a.jpg'\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    ldst = DEST+'fold_{}/train/'.format(i)\n",
    "    k_labels = []\n",
    "    for j in range(10):\n",
    "        fold = ldst+'fold_{}'.format(j)\n",
    "\n",
    "        fold_tr = fold+'/train/'.format(i)\n",
    "        fold_te = fold+'/test/'.format(i)\n",
    "        print(fold_tr)\n",
    "        k_features_train = features_extraction(fold_tr)\n",
    "        k_features_test = features_extraction(fold_te)\n",
    "\n",
    "        c_y = k_features_test.shape[1]\n",
    "        x_test = k_features_test.drop(c_y-1,axis=1)\n",
    "        x_train = k_features_train.drop(c_y-1,axis=1)\n",
    "        y_test = k_features_test[c_y-1]\n",
    "        y_train = k_features_train[c_y-1]\n",
    "\n",
    "        svm = SVC()\n",
    "        svm.fit(x_train,y_train)\n",
    "        pred = svm.predict(x_test)\n",
    "\n",
    "        k_labels.append(pred.ravel())\n",
    "    np.save(ldst+'features_'+'fold_{}'.format(i),np.array(k_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
